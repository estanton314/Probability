---
title: "*Positively* Everything You'll Ever Need to Know About the **Negative Binomial Distribution**"
author: "Eliot Stanton"
date: "November 2021"
output: 
  html_document:
    theme: flatly
    toc: true
    toc_float: true
    code_download: true
---

```{r message=FALSE, warning=FALSE, include=FALSE}
# Load any R Packages you may need
library(dplyr)
library(ggplot2)
library(plyr)
```


# What is the Negative Binomial Distribution?

*Note: In this tutorial, I'll call the Negative Binomial Distribution the NBD for short. By the time you're done learning, it really will seem like No Big Deal!*

### The Basics

#### Review of Bernoulli Trials

The NBD applies to a sequence of *Bernoulli* trials, meaning each trial:

- is *independent* from all other trials
- has a *binary* outcome- "success" or "failure"
  - remember, a "success" is the *outcome of interest*. it could still be something we'd societally consider a failure, like the death of a patient or malfunction of a car
- has the same constant success probability, *p*
  - since the outcome is binary, this means the probability of a failure is *1-p*

Often, we have a sequence of *multiple* Bernoulli trials... 

#### On to NBDs!

The Negative Binomial Distribution models a discrete random variable measuring the **number of the trial in a sequence on which the *rth* success occurs.** 

- Ex. how many coin tosses does it take to get 7 Heads?

The NBD takes two parameters: *r*, the number of successes we're looking for, and *p*, the success probability. 

- Ex. we're interested in r = 7 Heads, with a p = 0.5 chance of getting Heads on each trial.

Notation follows the format $Y \sim {NBin}(r, p)$, where Y is a random variable.

- Ex. to get the trial number of the 7th coin toss that lands on Heads, we'd investigate the distribution $Y \sim NBin(7, 0.5)$.


### When is it relevant?

Here are just a few examples of when the Negative Binomial Distribution might come in handy.


#### Example 1: Sports

- Data show that a soccer team is very likely to win if they score at least 3 goals. Their shot conversion rate (the proportion of shots that score) is 10%. If they get 20 shots in their next game, what is the probability that they score 3 or more goals?

#### Example 2: Gambling

- In a game, you pay \$2 to spin a wheel and get \$10 if you land on a winning spot. Your chance of winning is 20% - one in five spots are winners. You decide to stop playing after you win 5 times. What is the probability you break even?

# What are the Properties of the Negative Binomial Distribution?

The NBD typically models phenomena where we are interested in a *set number* of successes and want to measure the *counts* of trials it takes to achieve those successes.

### Probability Mass Function

The Probability Mass Function, or PMF, for the NBD is as follows:

- $P(Y=y) = \binom{y-1}{r-1} p^r (1-p)^{y-r}$

We can use the PMF to find the probability of our random variable taking on different values of $y$.

The *support*, or valid values, of $y$ for the PMF is $y =r,r+1,r+2,r+3...$

- it starts at r because you need a minimum of r trials to achieve r successes
- y can only take on integers because Y is a discrete random variable - we can't have a fraction of a success

### Expected Value and Variance

The expected value of a Negative Binomial Distribution is $E(Y) = \frac{r}{p}$.

The variance of the NBD is $Var(Y) = \frac{r(1-p)}{p^2}$.

### Connections to Other Distributions

#### Geometric

Recall that the geometric distribution models the trial number of the first success. 

- So, the geometric distribution is a special case of the NBD where r = 1.
  - Look at the NBD PMF. What happens if you set r equal to 1? Surprise- you get the geometric PMF!
- We can think of the NBD as the sum of r separate geometric distributions.
  - Look at the NBD expected value and variance. Notice that each one is the same as you'd expect for a geometric distribution but multipled by r.

#### Binomial 

Recall that the binomial distribution models the number of successes in n trials.

- So, the first y-1 trials of the NBD are actually a binomial distribution with r-1 successes.
  - Can you see the connection in the NBD PMF? It's a binomial PMF (for r-1 successes out of y-1 trials) multipled by the probability p of the final (rth) success!

# Visualizations

By now, I'm sure you're curious what the Negative Binomial Distribution looks like!

Let's go back to our example about a coin toss. What does the NBD look like for getting 7 Heads from a fair coin?

```{r echo=FALSE}
ggplot(mapping=aes(x=rnbinom(10000,7,.5)+7,y=..density..)) +
    geom_histogram(binwidth=1, colour="black", fill="white") +
  labs(x="Y ~ NBin(7,0.5) ",y="P(Y=y)")
```

The distribution is unimodal and skewed right.

#### Varying p

How would the NBD for r=7 change if we weighted our coin to be biased and changed the value of p? Let's look at p= 0.1, 0.2, 0.25, and 0.50.

```{r echo=FALSE}
dat <- data.frame(p = factor(rep(c(".1",".2",".25",".50"), each=10000)), 
                   NBD = c(rnbinom(10000,7,.1)+7,rnbinom(10000,7,.2)+7,rnbinom(10000,7,.25)+7,rnbinom(10000,7,.5)+7))

ggplot(dat, aes(x=NBD, color=p)) + geom_density() +
  labs(x="Y~NBin(7,p)",y="P(Y=y)")
```

You can see that as p *increases*, the center of the distribution shifts left (decreased expected value) and the peak gets taller (decreased variance). When Heads becomes a more likely outcome, we can probably get 7 of them in fewer trials! 

#### Varying r

How would the NBD change if we returned to our fair coin with p=0.5, but looked for a different number of successes? Let's try r= 5, 10, 25,and 50.

```{r echo=FALSE}
dat <- data.frame(r = factor(rep(c("5","10","25","50"), each=10000),levels=c("5","10","25","50")), 
                   NBD = c(rnbinom(10000,5,.5)+7,rnbinom(10000,10,.5)+7,rnbinom(10000,25,.5)+7,rnbinom(10000,50,.5)+7))
ggplot(dat, aes(x=NBD, color=r)) + geom_density() +
  labs(x="Y~NBin(r,0.5)",y="P(Y=y)")
```

What happens when r *increases*? The center of the distribution shifts right (increased expected value), and the peak flattens (increased variance). When we're looking for a greater number of Heads, we'll probably need more trials to get enough!

# Let's Simulate Together

Since I'm sure you're tired of coin tosses by now.... let's talk corn. You don't have time to check the ears at the grocery store, but you need 10 good ones for dinner tonight. 85% of them are good, and the rest are duds.
What do we know so far?

- We want 10 good ears, so **r=10**!
- 85% of the ears are good, so **p=.85**!
- So, our distribution is $Y \sim {NBin(10,.85)}$, where Y is the random variable measuring the number of trials it takes to get 10 good ears of corn.

#### rnbinom()

The first step is to figure out what one run of this experiment would look like. We want to pick ears of corn until we have 10 good ones.
```{r echo=TRUE}
corn = rnbinom(1,size=10,prob=.85)+10
```
```{r echo=FALSE}
print(paste("This time, it took", corn, "ears of corn for us to find 10 good ones."))
```


What does this code mean? Here's how $\sf rnbinom(n,size,prob)$ works.

- n is the number of times we run the experiment
  -we started with just 1
- size is r, the number of successes we want to see
  -in this case, 10
- prob is p, the probability of success
  -in this case, .85
- the output of $\sf rnbinom$ is *the number of failures before the rth success*, so we have to **add r** to get the trial number of the rth success, or $Y$
  -we add 10!
  
Now, let's run the experiment 10000 times, and look at our results! First, we'll check out the first 10 runs and overall maximum and minimum. Then, we look at the mean, which is an estimate for the expected value $E(Y)$, and compare it to the actual value. Finally, we graph our results to see the distribution of all 10000 simulated grocery trips.
```{r echo=TRUE}
corns = rnbinom(10000,size=10,prob=.85)+10
```
```{r echo=FALSE}
print("Here's the value of Y for the first ten runs of our experiment:")
head(corns,10)

print(paste("The minimum was",min(corns),"and the maximum was",max(corns)))

print("The mean of Y in our simulation is an estimate for the expected value of Y.")
print(paste("Our simulation mean",round(mean(corns),3),"did pretty well compared to the actual expected value, E(Y) = r/p = 10/.85 =",round(10/.85,3)))

ggplot(mapping=aes(x=corns,y=..density..)) +
    geom_histogram(binwidth=1, colour="black", fill="white") +
  labs(x="Y ~ NBin(10,0.85) ",y="P(Y=y)") + geom_vline(xintercept=mean(corns),color="red",linetype="dashed")
```

Now, let's do a few problems with simulated data, and compare our results to exact functions in R!

### P(Y=y)

#### Simulation-

What's the probability we find the 10th good ear of corn in 10 tries, with no duds at all? We can check how many of our 10000 simulated experiments had Y=10!
```{r echo=TRUE}
corns = rnbinom(10000,size=10,prob=.85)+10
ten = corns==10
```


```{r echo=FALSE}
print("Here are the first ten results- did Y=10?")
print(head(ten,10))
print(paste("The simulated probability P(Y=10) is",round(mean(ten),3)))
```

- Notice that I checked each experimental "Y" to see if it equaled 10 and made a vector of TRUE and FALSE values
- Since TRUE is stored as 1, and FALSE is stored as 0, taking the mean of them all gives the proportion that were TRUE, which is our estimated probabiliy!

#### Exact Solution- 

Want to solve a problem of the form $P(Y=y)$? Use $\sf dnbinom(x,size,prob)$!

Again, I ask: what's the probability we find the 10th good ear of corn in 10 tries, with no duds at all?
```{r echo=TRUE}
ten = dnbinom(0,10,.85)
```


```{r echo=FALSE}
print(paste("The exact P(Y=10) =",round(ten,3)))
```
How did I do that, you ask? Here's how $\sf dnbinom(x,size,prob)$ works!

- x is the number of *the number of failures before the rth success*, so we have to **subtract r** from the $y$ we are interested in
  -our y=10 and our r=10, so x= 10-10 = 0
- size is r, the number of successes we want to see
  -in this case, 10
- prob is p, the probability of success
  -in this case, .85
- the output of $\sf dnbinom$ is *the probability of our rth success happening ON trial y*

### P(Y≤y)

#### Simulation-

What are the chances we get 10 good ears if we buy 12 just to be safe? We can check how many of our 10000 simulated experiments had Y≤12!
```{r echo=TRUE}
corns = rnbinom(10000,size=10,prob=.85)+10
twelveorless = corns<=12
```


```{r echo=FALSE}
print("Here are the first 10 results- was Y≤12?")
print(head(twelveorless,10))
print(paste("The simulated probability P(Y≤12) is",round(mean(twelveorless),3)))
```

- We used the same strategies here as we did before- making a vector of TRUEs and FALSEs, and taking its mean.

#### Exact Solution-

Maybe we want to solve a problem requiring  $P(Y \le y)$... Let's use $\sf pnbinom(q,size,prob)$!

Again, I ask: what are the chances we get 10 good ears if we buy 12 just to be safe?
```{r echo=TRUE}
twelveorless = pnbinom(2,10,.85)
```


```{r echo=FALSE}
print(paste("The exact P(Y≤12) is",round(twelveorless,3)))
```
Here's how $\sf pnbinom(q,size,prob)$ works!

- q is the number of *the MAX number of failures we'll allow before the rth success*, so we have to **subtract r** from the $y$ we are interested in
  -our y=12 and our r=10, so q= 12-10 = 2
- size is r, the number of successes we want to see
  -in this case, 10
- prob is p, the probability of success
  -in this case, .85
- the output of $\sf pnbinom$ is the *probability of our rth success happening ON OR BEFORE trial y*


### An Aside: Percentiles of the NBD

#### Exact Solution-

If we're faced with a problem where we already have $P(Y \le y)$ and we want to solve for y, we can use $\sf qnbinom(p,size,prob)$.

For example, what if we want to be 95% sure we'll get 10 good ears?  
```{r echo=TRUE}
ninetyfive = qnbinom(.95,10,.85)+10
```


```{r echo=FALSE}
print(paste("The value of y at the 95% percentile is",round(ninetyfive,3)))
```

Here's how $\sf qnbinom(p,size,prob)$ works!

- p is the *percentile* of the distribution that we want to find $y$ for
- size is r, the number of successes we want to see
  -in this case, 10
- prob is p, the probability of success
  -in this case, .85
- the output of $\sf qnbinom$ is the *value of y with a p% chance that we'll find our rth success on or before it*


# Put Your Knowledge to the Test!

Whew, that was a lot! Now that we've covered *positively* everything you need to know about the Negative Binomial Distribution, it's your turn to see if you can use your new skills!

Remember, you have many tools at your fingertips:

- the PMF formula
- expected value and variance formulas
- simulations with $\sf rnbinom$
- calculations with $\sf dnbinom$, $\sf pnbinom$, and $\sf qnbinom$

### Sample Exercise

Imagine you work for a professional basketball player, Moose, one of the leading stars in the WNBA. Moose is trying to improve her free throw shooting at the request of her Coach, Anthony. Assume all of Moose's shot attempts are independent from each other.

#### A)

Right now, Moose's free throw percentage is 65%. Coach Anthony says he needs her to make 6 free throws per game. What is the probability that Moose will hit her 6th free throw on her 10th attempt? Can you solve this using both the proper formula and the proper R function?

#### B)

Find the expected value and variance for the number of free throw attempts it will take for Moose to hit 6. Use formulas, not simulation.


#### C)

What is the probability that it takes Moose more than 4 attempts to hit her first 3 free throws?


#### D)

Simulate 10000 instances of the upcoming season, where each season is 30 games. Assume Moose attempts exactly 10 free throws per game, and all games are independent of each other. Simulate the average number of games in a season that Moose will hit her target of 6 successful free throws. 
BONUS: Plot your results! You can use the following shell for your code:

- ggplot(mapping=aes(x=***simulation results***,y=..density..)) +
    geom_histogram(binwidth=1, colour="black", fill="white") +
  labs(x="Number of Games Where Moose Hits 6 FTs",y="probability of x")

### Solutions

#### A)

- r = 6
- p = .65
- We're trying to solve P(Y=10)
  - Use the PMF formula: $P(Y=y) = \binom{y-1}{r-1} p^r (1-p)^{y-r}$
  - Or, use $\sf dnbinom(y-r,r,p)$

```{r}
dnbinom(10-6,6,.65)
#OR 
choose(10-1,6-1)*(.65)^6*(1-.65)^(10-6)
```

#### B)

- r = 6
- p = .65
- We're trying to find E(Y)
  - $E(Y) = \frac{r}{p}$
- We're also trying to find Var(Y)
  -$Var(Y) = \frac{r(1-p)}{p^2}$

```{r}
6/.65
(6*(1-.65))/(.65^2)
```

#### C)

- r = 3
- p = .65
- We're trying to find P(Y>4), which is the same as 1-P(Y≤4)
  - Use the PMF formula for P(Y=3)+P(Y=4)
  - Or, use $\sf pnbinom(y-r,r,p)$

```{r echo=TRUE}
1-(choose(3-1,3-1)*(.65)^3*(1-.65)^(3-3)+choose(4-1,3-1)*(.65)^3*(1-.65)^(4-3))
#OR
1-pnbinom(4-3,3,.65)
```

#### D)

- rnbinom(30,...) to get the attempt number of the 6th successful shot
  - if ≤ 10, she hit her target for the game
- replicate(10000,{....})
- r=6
- p=.65
- Use rnbinom(#,r,p)+r

```{r echo=TRUE}
seasons = replicate(10000,{
  games = rnbinom(30,6,.65)+6
  target = games<=10
  sum(target)
})
print(mean(seasons))

ggplot(mapping=aes(x=seasons,y=..density..)) +
    geom_histogram(binwidth=1, colour="black", fill="white") +
  labs(x="Number of Games Where Moose Hits 6 FTs",y="probability of x")
```

#### Thanks for Reading!!!


```{r eval=FALSE, include=FALSE}
#citing some sources
#https://rpubs.com/mpfoley73/458738
#http://www.cookbook-r.com/Graphs/Plotting_distributions_(ggplot2)/
```



